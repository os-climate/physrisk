\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}

\title{OS-Climate documentation - Heat}
\author{mariem.bouchaala}
\date{September 2022}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Generic Methodology for Physical Climate Risk Modelling
%
% 2021 OS-Climate
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[a4paper,11pt]{extarticle} %12pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Required packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{framed}
\usepackage{glossaries}
\usepackage{graphicx}
\usepackage[colorlinks,citecolor=blue,urlcolor=black,linkcolor=black,bookmarks=false,hypertexnames=true]{hyperref}
\usepackage{numprint}
%\usepackage{physics}  % causing problems; using different notation for bra-ket
%\usepackage{sfmath}[cmbright]
%\usepackage[round]{natbib}
\usepackage{ragged2e}
\usepackage{scrextend}
\usepackage{sistyle}
\usepackage{subcaption}

\usepackage{bookmark}

\usepackage[normalem]{ulem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% General settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%
% Font
%%%%%%%%%%%%%

%\renewcommand*{\familydefault}{\sfdefault}

%%%%%%%%%%%%%
% Spacing
%%%%%%%%%%%%%

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}
\frenchspacing

%%%%%%%%%%%%%
% Hyphenation
%%%%%%%%%%%%%

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%%%%%%%%%%%%%
% Number formatting
%%%%%%%%%%%%%

\npthousandsep{,}
\npthousandthpartsep{}
\npdecimalsign{.}

%%%%%%%%%%%%%
% Footnotes
%%%%%%%%%%%%%

\usepackage[hang, flushmargin]{footmisc}
\setlength{\footnotemargin}{4mm}

%%%%%%%%%%%%%
% Running title
%%%%%%%%%%%%%

\pagestyle{fancy}
\lhead{OS-Climate}
\rhead{Physical Climate Risk Methodology}

\makeglossaries


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Physical Climate Risk Methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Joe Moorhouse\thanks{\textit{E-mail}: Joe.Moorhouse@gmail.com}
        \and
        Florian Gallo\thanks{}
        \and
        Mariem Bouchaala\thanks{{E-mail}: mariem.bouchaala@essec.edu}
        \and
        Davide Ferri\thanks{\textit{E-mail}:davide.ferri.94@gmail.com
        \smallskip
        \newline% \indent
    The views expressed in this paper are those of the authors and do not necessarily reflect the views and policies of their respective employers.}
    }

\date{October 2022 [Draft]}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle{}

%\begin{abstract}
%Add abstract here.
%\end{abstract}


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TEMP ONLY during writing stage
\setcounter{tocdepth}{4}
\renewcommand{\contentsname}{Contents}
\tableofcontents

%\listoftables

%\listoffigures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{Sec:Introduction}

The changing climate introduces new risks. These can be grouped into:
\begin{enumerate}
	\item Physical risks -- risks arising from the physical effects of climate change
	\item Transition risks -- risks arising from the transition to a low-carbon economy\footnote{Liability risks are considered a third by some\cite{WoetzelEtAl:2020}. These are risks arising from those affected by anthropogenic climate change seeking compensation.}   
\end{enumerate}


The methodology presented in this document concerns the assessment of physical risk. Physical risk comes from changes in climate \emph{\gls{hazard}s}, climate-related physical phenomena that have the potential to impact natural and socioeconomic systems \cite{WoetzelEtAl:2020}\cite{MitchellEtAl:2017}. Hazards can be divided into \emph{\gls{acute_hazard}s} and \emph{\gls{chronic_hazard}s}. Acute hazards are those associated with \emph{events}, for example heat waves, inundations (floods) and hurricanes. Chronic hazards are long-term shifts in climate parameters such as average temperature, sea-level or water stress indices.  


\newglossaryentry{hazard}
{
	name=hazard,
	description=Climate-related physical phenonenon that can impact natural and socioeconomic systems.
}
\newglossaryentry{acute_hazard}
{
	name=acute hazard,
	description={Hazard which is an event, for example a heat wave, inundation, hurricane or wild fire.}
}
\newglossaryentry{chronic_hazard}
{
	name=chronic hazard,
	description={Hazard which is a long-term shift in a climate parameter such as average temperature, sea-level or a water stress index.}  
}
\newglossaryentry{hazard_event}
{
	name=hazard event,
	description={Definition of the occurrence of a hazard which can be assigned a probability. For example, a flood occurring in the year 2050 at a certain location with a depth greater than 50 cm.}
}
\newglossaryentry{hazard_parameter}
{
	name=hazard parameter,
	description=Definition of the shifting parameter of a chronic hazard. This is a time-varying quantity such as the average temperature or the average number of days per year over a certain temperature threshold. 
}


A model designed to quantify physical risk must take into account: a) hazards' likelihood of occurrence  b) the damage or disruption caused by a hazard c) the consequence of this damage/disruption. Damage/disruption caused by a hazard is determined by the \emph{vulnerability} of the asset that is exposed\footnote{\emph{Exposure} of an asset to a hazard is defined after \cite{MaskreyEtAl:2011}; for most purposes this is determined by asset location.}. With a focus on financial risk, damage/disruption refers to damage of financial assets and disruption to business activities. More generally, damage can refer to natural assets and disruption to populations and ecosystems. Hereafter we use the word `asset' to describe both physical assets and business activities. As an example, the physical infrastructure of a power generating asset may be damaged by inundation and its electricity production may be disrupted, leading to a loss in revenue. 

We assign explicit names to these three components, a), b) and c) for the financial risk case:
\begin{enumerate}[a)]
\item Hazard
\item Vulnerability
\item Financial
\end{enumerate}

A precise model of the physical risk from a single (localized) asset or business activity must consider a hazard's likelihood of occurrence \emph{at the asset's locale} (i.e. if the asset is exposed to the hazard) and vulnerability to the hazard particular to that asset. This requires specific knowledge of the asset. For example a power station that relies on air-cooling might be disrupted by a period of extremely high air temperature. In addition, an asset may be impacted through its reliance on other assets; a manufacturing facility may rely on continuity of electricity supply for example.

Such precise models of physical risk can be complex and may rely on information that is not readily available. For these reasons, approximations are commonly used, although approximate models still typically include hazard, vulnerability and financial components \cite{BertramEtAl:2020}\cite{WoetzelEtAl:2020} -- even highly-approximate global-scale impact analyses used in macroeconomic models.

The purpose of this paper is to present the methodology of a framework that is sufficiently generic to be used for a wide range of physical climate risk models, both precise and approximate as required. The ability to perform precise, fine-grained calculations is an important requirement therefore. This paper serves as a specification for use in the \emph{`physrisk'} OS-Climate (OS-C) \cite{OSC} physical climate risk calculation module.

OS-C aims to provide an platform unconstrained by any one particular methodology choice, but takes inspiration from natural catastrophe modelling \cite{MitchellEtAl:2017} and in particular the \emph{Oasis Loss Modelling Framework} \cite{OasisLMF} (henceforth \emph{Oasis}), which was designed to accommodate a wide range of catastrophe models and analyse physical risk in the context of the insurance market. Similarly to \emph{Oasis}, we adopt a modular approach. This approach allows the user to change easily a particular modelling method, whilst maintaining the integration of the components. 

In the following, models of hazards, vulnerability and financial impact are discussed in more detail. In a later section these are presented more formally.

\subsection{Hazard model}
Hazard models come in two varieties: models of acute hazards and models of chronic hazards.
\begin{enumerate}[label=\Alph*.]
\item{\emph{Accute hazard models.}}
Models of accute hazards provide probabilities that accute hazards of a certain intensity occur within a given future time period. For example a hazard model may provide the probability that an inundation occurs in a certain location in th year 2050 with a flood depth greater than 30~cm. 

Accute hazard models may or may not be \emph{event-based}
\paragraph{Event-based hazard models}
What we term `event-based hazard models' are the models common in natural catastrophe modelling. These provide a spatial distribution of hazard probabilities -- the `hazard footprint' -- conditional on the occurrence of some catastrophic event. This event might be, for example, a Noth American hurricane, a European heat wave or a South Asian inundation. 

Event-based hazard models are important when the \emph{correlation} of hazards to which assets are exposed is material to the analysis being performed. Non-event-based hazard models provide probabilities that accute hazards occur in a certain location but with no information about how likely it is that two assets may experience the hazards at the same time. For example, if one house on a street is exposed to an inundation it is quite possible that the house two doors down will also be exposed. This is captured by event-based hazard models when both houses appear in the same event footprint.

Of the face of it, non-event-based hazard models may appear to be missing important information  



\item{\emph{Chronic hazards.}}
Risk factors are modelled using probability distributions calibrated to historically-observed returns; simplifying assumptions such as a multivariate normal distribution are made in order to obtain closed-form expressions for the VaR.

Risk factors are again reflected by probability distributions calibrated to historically-observed returns. The joint distributions are typically fitted to historical data more closely than in the case of a Parametric~VaR and, as such, usually do not allow for closed-form expressions.

\end{enumerate}

\begin{enumerate}[a)]
	\item For \emph{acute hazards} distributions 
	\item Vulnerability
	\item Financial
\end{enumerate}
  such as the probability of an inundation above a certain depth, or a period of drought, in a specific area. 

Models of acute hazards produce probability distributions for future events whereas chronic hazard models produce climate parameters only. Hazard models are pure climate models, whose outputs can be used as an input for the next steps of the analysis. 

\paragraph{Vulnerability model}
The vulnerability component measures the potential impact of a catastrophic event on an asset. Vulnerability models, like acute hazard models, are probabilistic.  loss of output suffered by my power station if an inundation of a certain intensity happens?". 

Finally, the Financial module is concerned with translating the asset damage to a loss of profitability for the company, or a loss of value for a lender, insurer, equity stakeholder etc. Clearly, the models used in the Financial module answer questions specific to a certain user: is the ultimate objective that of measuring the physical risk for the company or for one of the asset's insurer? Depending on the answer to that question, a different Financial model might be needed. 
At time of writing, physical risk calculations may make use of `bulk-assessment' approaches where accurate asset vulnerability information is unavailable and approximations are therefore required. The modelling framework accommodates bulk-assessment-type models as well as approaches capable of modelling vulnerability more precisely\footnote{There is potentially great value in the results obtained from very simple models, as long as the model error can be quantified. The aim is to be able to accommodate both simple and complex models in combination.}. The framework is designed to control the model risk that this creates by incorporating a model of the uncertainty of the approximations into the calculation.

 The different elements can be modelled in a variety of ways, under different approximations. However we consider that a `micro' or `fine-granularity' method is pre-requisite for all of these. For example, 

Hazard likelihood of occurrence is generally scenario-based \cite{BertramEtAl:2020}. 

A number of papers have developed models and tools that tackle one specific aspect of the problem. However, it is not currently easy to seamlessly integrate models which operate in different areas to produce an end to end physical risk analysis, or to experiment with different approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Literature}

\section{Design goal}
The design goal of the \emph{`physrisk'} library is to facilitate the analysis of physical risk from a variety of perspectives. There is no specific market (e.g. insurance market) in mind and we want to make sure that our framework is general enough to allow any stakeholder to reason about their physical risks. In addition, we want our framework to make as few assumptions as possible: The clear hypothesis is that we can treat the modules as dependent from one another only through their potential input-output relationships. Other than that, we want the user to have a flexible access to a wide choice of models within each module: the models are only required to have a predictable external behavior, while the details of the internal workings can be defined with flexibility.

\section{Model description}

\subsection{Overview}
A high-level view of the physical risk modelling framework is shown in Figure~\ref{Fig:top_level_view}.

Hazard models are used to create hazard data sets, providing probability distributions of events such as inundations, periods of drought or periods of high wind. These data sets might, for example, specify the annual probability of occurrence of an event (e.g. high wind) of a certainty intensity (e.g. maximum wind speed) for some specified year in the future.

Vulnerability models are used to construct, for a given set of assets, both:
\begin{itemize}
    \item Asset event distributions: probability distributions of events that impact the assets at their locations, derived from hazard data sets
    \item Vulnerability distributions: conditional probability distributions of the impacts on the assets of events of given intensity
\end{itemize}

The asset impact model uses these quantities to derive distributions of impact for each asset. An impact might be, for example, damage to the asset, expressed as a fraction of the asst value. The financial risk model calculates financial measures from the impact distributions, for example Exceedance Probability.

Within the OS-C modelling framework, models are interchangeable and allow forms of composition. That is, different choices of vulnerability model may be used for a particular asset and a vulnerability model may use different hazard data sets for its calculation. The intention is to allow a risk calculation to be built from an ecosystem of hazard and vulnerability models according to the requirements of the model owner.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]

    \begin{framed}
        % left, bottom, right, top
        %\includegraphics[clip, trim=0cm 7cm 0cm 1cm, width=1.00\textwidth]{plots/top_level_view.pdf}
        \includegraphics[clip, trim=0cm 7cm 0cm 1cm, width=1.00\textwidth]{top_level_view.pdf}


    \end{framed}

    \footnotesize

    \renewcommand{\arraystretch}{1.01}

    \vspace{-3ex}

    \vspace{-0.5ex}

    \caption{\small Physical risk model components. }
    \label{Fig:top_level_view}

\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Asset impact model}
\label{SubSec:AcuteAssetImpactModel}
The {\it asset impact} model is used to determine how an asset is impacted by an event. The impact is a quantity from which financial loss can be inferred, but is not itself a monetary value. For example, a response might be the damage sustained to a building as a fraction of its value or the annual loss of energy output of a power station as a fraction of its annual output\footnote{A systemic change in annual output changes asset value, since this is partly determined by the expected future cash flows generated by the asset.}. In each case, a further model is required to translate the impact to a change in asset value. In principle an impact might lead to an increase or decrease in value.

Catastrophe models sometimes define a quantity `damage', and talk about `damageability'. `Damage' and `impact' are analagous quantities here but `impact' is perhaps better-suited to situations where there is, say, a decrease in output efficiency of a plant as a result of a period of higher temperatures.

Asset impact models as used in physical risk calculations may overlap with those of catastrophe models. OS-C aims to support a wide range of models, but it is desirable to identify approaches that generalize a large class of these. One such approach is adopted from Oasis \cite{OasisLMF}. The first assumption behind this is that a model should capture two important types of uncertainty, doing so by representing each by a probability distributions:
\begin{enumerate}
    \item Uncertainty as to the frequency and intensity (or severity) of events that potentially lead to a change in asset value. This is sometimes called the {\it primary uncertainty}
    \item Uncertainty as to the vulnerability of assets to events (i.e. response of assets to events of a given intensity), the {\it secondary uncertainty}
\end{enumerate}

These quantities are defined more precisely in \ref{Sec:MathematicalDescriptionOfAssetImpactModel}. Impact can be modelled using a {\it mean impact curve} (or {\it mean damage curve} in catastrophe modelling nomenclature). This is a curve relating an event intensity to an impact (e.g. a wind event with a given maximum gust speed will cause a given fractional damage to a property). In general, however, there is uncertainty as to the impact on an asset to an event of a given intensity -- in the example, the wind may cause mild or severe damage. For this reason, the vulnerability is represented rather as a two dimensional curve.

A second assumption is that the probabilities of such events may not be readily represented by distributions such as beta, gamma, beta-Bernoulli or truncated Gaussian and may be complex and multi-modal. Discrete probability distributions are therefore used in order to represent the range of possible distributions: a non-parametric approach.

\subsubsection{Mathematical description of asset impact model}
\label{Sec:MathematicalDescriptionOfAssetImpactModel}

There are $n$ intensity bins with index $i$ such that $i \in \{1, \dots, n \}$. We define $e^{(a)}_i$ to be the probability that a hazard event of type $a$ occurs with an intensity that falls in bin $i$. If $S^{(a)}$, a random variable, is the intensity of event $a$ then:

\begin{equation}
    \label{Eq:event}
    e^{(a)}_i = P \left( s^{(a, \text{lower})}_i < S^{(a)} \le s^{(a, \text{upper})}_i \right)
\end{equation}

That is, $s^{(a, \text{lower})}_i$ and $s^{(a, \text{upper})}_i$ define the range of bin $i$.

We define $v^{(a, b)}_{ij}$ to be the conditional probability that \emph{given} the occurrence of an event of type $a$ with intensity $S^{(a)}$ there is an impact (typically a damage or disruption\footnote{$d$ for `damage/disruption' is used to denote impact as $i$ is reserved for indexing}), $D^{(b)}$ in the range $d^{(a,b,\text{lower})}_j < D \le d^{(a,b,\text{upper})}_j$. The impact is of type $b$.


\begin{equation}
    \label{Eq:vulnerability}
    v^{(a, b)}_{ij} = P \left( d^{(a,b,\text{lower})}_j < D^{(b)} \le d^{(a,b,\text{upper})}_j | s^{(a, \text{lower})}_i < S^{(a)} \le s^{(a, \text{upper})}_i \right)
\end{equation}

The definition of an event type $a$ includes a time interval e.g. $a$ is the occurrence of an inundation in the locale of the asset {\it within a one year period}. $b$ is, for example, the fractional damage to the asset.

We define $d^{(a,b)}_j$ to be the marginal probability of impact $D^{(b)}$ in the range $d^{(a,b, \text{lower})}_j < D^{(b)} \le d^{(a,b,\text{upper})}_j$ occurring as a result of an event of type $a$.

\begin{equation}
    \label{Eq:impact}
    d^{(a,b)}_j = P \left( d^{(a,b,\text{lower})}_j < D^{(b)} \le d^{(a,b,\text{upper})}_j \right)
\end{equation}

From the definition of conditional probability:

\begin{equation}
    \label{Eq:model}
    d^{(a,b)}_j = \sum_{i} v^{(a,b)}_{ij} e^{(a)}_i
\end{equation}

If only the mean impact curve is available, then it is possible to create the matrix such that $v_{ij} \in \{0, 1\}$. The matrix then provides a simple mapping from intensity to impact; if the number of intensity and response bins is equal then matrix $\mathbf{v}$ is simply the identity matrix. However, note that these simplifications exclude from the model any uncertainty in the parameters\footnote{A better approach would be to estimate the standard deviation of the distributions from which the mean impact curve was calculated and to incorporate this.}.

Note that $d^{(a,b)}_j$ is identical to the {\it effective damage} distribution of Oasis and can be described as the `effective impact'. It is a marginal distribution and does not capture any correlation between events nor impacts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]

    \begin{framed}

        %\includegraphics[width=\textwidth]{plots/fig_intensity.pdf}
        \includegraphics[width=\textwidth]{fig_intensity.pdf}

    \end{framed}

    \footnotesize

    \renewcommand{\arraystretch}{1.01}

    \vspace{-3ex}

    {\justify
        The exceedance curve of event intensity at the asset location is shown on the right. The event intensity in this example is inundation depth in metres. Exceedance is a cumulative probability. As an example, the probability of an inundation event occurring within a single year of intensity 0.91m or greater is 0.002. An exceedance probability is the reciprocal of the return period; it could equivalently be said that the 0.91m intensity event occurs with a return period of 500 years.
        The exceedance curve can be converted to a histogram of probabilities. Here the $n$ bins have ranges $[s^{(a, \text{lower})}_i, s^{(a, \text{upper})}_i]$. For example, the first bin has range [0.28m, 0.38m]. The second bin has range [0.38m,    0.51m]; that is $s^{(a, \text{lower})}_2 = 0.38$m and $s^{(a, \text{upper})}_2 = 0.51$m. $e^{(a)}_2 = 0.06$.
        \par}

    \vspace{-0.5ex}

    \caption{\small Event intensity exceedance curve (right) and corresponding histogram (left).}
    \label{Fig:intensity}

\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Importance of secondary uncertainty}
The importance of the vulnerability matrix as opposed to mean damage curve (or vector) is emphasized above; see also \cite{Taylor:2015} for a discussion of this point. This is true not only in cases where the underlying distribution of an impact, for example a fractional damage, can be inferred from empirical data; see for example Figure~\ref{Fig:vulnerability_matrix}). This is arguably \emph{more} important where data is limited in order that approximate data can be incorporated into the model in a way that the impact of the approximations can be well-understood.

Vulnerability data may be provided by
\begin{itemize}
    \item Modelling of asset vulnerability based on asset characteristics and/or historical data
    \item 'Calibrated' vulnerabilities, for example based on realized insurance claims
\end{itemize}
Physical risk models may make use of so-called `bulk assessment' approaches for certain assets, where precise vulnerability information is not available and less precise estimates of the damage/disruption of the asset are used. The presence of such estimates in an overall model may, or may not, materially impact the accuracy of the results, but it is important that this impact can be assessed. By quantifying the uncertainty in the response estimates, a distribution of financial losses is ultimately obtained from which the model user can derive the impact of the approximation.

\paragraph{Handling epistemic uncertainty}
In forms of bulk-assessment, a common case is that insufficient information exists with which to characterize an asset. This is an example of an epistemic, as opposed to aleatory, uncertainty. The epistemic uncertainty, and its impact, can be included in the model in a relatively straight-forward way.

We extend Equation \ref{Eq:vulnerability}, by including a new discrete random variable, $A$, which is the type of the asset.
\begin{equation}
    \label{Eq:vulnerability}
    v^{(a, b)}_{ij} = P \left( d^{(a,b,\text{lower})}_j < D^{(b)} \le d^{(a,b,\text{upper})}_j | s^{(a, \text{lower})}_i < S^{(a)} \le s^{(a, \text{upper})}_i, A = a_1 \right)
\end{equation}


\subsubsection{Interpolation of probability distributions}
Cases arise where the event distributions and vulnerability distributions are not defined for a common set of intensity bins and interpolation is therefore required. The question then arises of how probability density is distributed within bins. The choice is model-specific and customizable, but here two common cases are described.

\begin{itemize}
    \item Probability density constant across bin: linear interpolation of cumulative probability function
    \item Probability density changes linearly across bin: quadratic interpolation of cumulative probability function
\end{itemize}

{\textcolor{red}{\emph{[Add equations and example plots here]}}}

Hazard data sets might also contain instances of `point-probabilities', for example where there is a finite probability that the intensity of an event takes a single value. These represent Dirac delta functions in the probability distribution, steps in the cumulative probability function. There is the option of retaining these as delta functions (bins of zero width), but in some cases it may be necessary to make assumptions about how these the probability might be distributed across a bin.

{\textcolor{red}{\emph{[Add equations and plot of step-CDF with interpolation; exemplify by `damage threshold']}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]

    \begin{framed}

        %\includegraphics[width=\textwidth]{plots/vulnerability_lagace_2008.png}
    \includegraphics[width=\textwidth]{vulnerability_lagace_2008.png}

    \end{framed}

    \footnotesize

    \renewcommand{\arraystretch}{1.01}

    \vspace{-3ex}

%    {\justify
%        Taken from
%        \par}

    \vspace{-0.5ex}

    \caption{\small Taken from Lagacé (2008) Catastrophe Modeling, Université Laval. Mean damage curve as an approximation to an underlying set of distributions, modelled using a vulnerability matrix. {\textcolor{red}{\emph{[To seek permission or replace e.g. with synthetic plot]}}}}
    \label{Fig:vulnerability_matrix}

\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Effective impact distribution}
$d^{(a,b)}_j$ from Equation~\ref{Eq:vulnerability} is the probability distribution of impacts of type $b$ for an asset as a result of events of type $a$. In the catastrophe models of Oasis, impacts are sampled from this distribution \cite{OasisFinancialModule}, for example samples of fractional damage, which form the basis of a Monte Carlo calculation. This is done in order to apply insurance policy terms and conditions which can be complex and non-linear.

The Monte Carlo sampling is done by constructing a cumulative probability density function, $Y_D(d)$, of impact $D$ from the effective impact distribution ($Y_D(d) = P(D \le d$)). Random numbers, $u_i$ are then sampled from a standard uniform distribution ($u_i \in [0, 1]$), from which impacts are calculated by:

\begin{equation}
    \label{Eq:sampling}
    d_i = Y^{-1}_D(u_i)
\end{equation}

In this Monte Carlo approach, samples of fractional damage can be drawn from distributions so as to be correlated or uncorrelated. For example, if the impact distributions represent damage to buildings as a result of inundation then it may be appropriate to model damage to two buildings in close proximity as being highly correlated\footnote{Catastrophe model practitioners might point out that presence or absence of kerb stones and availability of sand bags are highly significant so any such assumption is prone to error}. If the buildings are far apart (say in different countries) then the correlation is likely to be close to zero.

\subsubsection{Full Monte Carlo calculation}
A more sophisticated correlation model might try to capture correlation of events and of vulnerabilities. Such models would typically need to first sample from the distribution of event intensity and then from the vulnerability distribution. This is more computationally expensive than the approach of deriving an effective impact distribution. Such a `full Monte Carlo' approach might prove to be relevant for some models as it is a highly flexible approach.


\subsection{Aggregation of impacts}
For impacts of the same type, $b$, arising from different events, it is assumed that the impacts are additive, up to a ceiling value\footnote{this approximation is only strictly valid for sufficiently small impacts; consider the contrived example of 0.8 fractional damage that occurs from both flood and high wind in the same year.}. If the annual impacts from events with index 1 and 2 are represented by random variables, $Y^{(1,b)}$, $Y^{(2,b)}$ then $Y^{(\text{tot}, b)} = Y^{(1,b)} + Y^{(2,b)}$.

If the random variables are uncorrelated, then the aggregated effective impact distribution is given by the convolution:

\begin{equation}
    \label{Eq:sampling}
    y^{(\text{tot}, b)}(r) = \int^{\infty}_{-\infty} y^{(1, b)}(t) y^{(2, b)}(r - t) dt
\end{equation}

{\textcolor{red}{\emph{[Add version with discrete binned data.]}}}

\subsection{Financial loss model}
Several financial measures are of interest.

\begin{enumerate}
    \item Annual Exceedance Probability (AEP): the probability that in a given year the aggregated losses of a portfolio will exceed a certain value
    \item Valuation Adjustment: an adjustment to the present value of an asset to reflect the expected loss
\end{enumerate}

The first of these typically requires less data in its calculation. This is a cumulative probability distribution of losses from which the average annual loss (AAL) can be inferred, but also the range of losses in a given confidence interval. This interval is driven by the primary and secondary uncertainties above.

With additional modelling steps, credit risk measures can also be derived.

\subsubsection{Structural models of credit risk}
Changes in asset value can be used to model changes in the credit quality of market participants. Financial risk modules for physical risk may then use distributions of asset value changes in order to model changes of credit quality over time as a result of climate change, for example estimates of default probability and loss given default.

The intention of this section is not to specify any particular model, but rather to give a brief introduction. Particularly of interest is the question of what inputs credit risk models require.

For medium and large cap firms, a credit default event typically occurs when a firm is not able to meet its debt servicing obligations. Under an important class of credit risk models called `structural models', it is assumed that a default event occurs for a firm when its assets are sufficiently low compared to its liabilities.

A number of different structural models exist which make various assumptions about how a firm's assets change over time, how its capital is structured and the nature of its debt.

The earliest structural model was described by Merton in 1974 \cite{Merton:1974} based on an extremely simple debt structure. Black and Cox \cite{BlackCox:1976} introduced an important refinement to the Merton model in 1976. Practical implementations were subsequently created as a result of this foundational work. A notable one of these is the `KMV' model, named after Kealhofer, McQuown and Vasiek, now owned by Moody's Investors Service, Inc.

Use of such credit models, may provide a mechanism for incorporation of physical risk into financial institutions existing risk models\cite{KenyonEtAl:2021}.


Loss given default (LGD). Do we need 


\subsection{Uncertainties in the calculation}

\subsection{Model limitations}

\begin{enumerate}
    \item Spatial correlation of events: to what extent possible without MC calculation; to what extent is provided / can be inferred from data sets
    \item Correlation of vulnerability
    \item Data availability
\end{enumerate}


\subsubsection{Data availability}
Issues related to data availability and relevance are still one of the main limitations of physical risks assessments. If past and future climate data are becoming increasingly available through open-sources portals and tools (e.g. Copernicus, WRI Aqueduct), their availability and their reliability varies widely according to the climate hazard of interest, the region and the modelling process. If the availability of climate data is improving, open-source, asset-level information (required to estimate the exposure of an asset to a give climate hazard) is still seldom available. Such data include the location of assets, their link with owning companies and more generally any damages records that could be used to quantify the response of an asset (or of a type of asset) to a given climate event. Newly-published datasets have been recently released for some sectors but their exhaustiveness remains to be verified. Moreover, many industrial sectors are not covered, thus limiting the application of physical risks methodologies to a diversified portfolio.
Finally, building and applying the correlation between hazard and damage (or impact), as described in section 2.2, requires common distribution between historical events, historical damages and future climate events. In a changing climate, assets and activities will be impacted by more intense events that will not have been experienced either in a given region of the world or even on the
whole globe, leading to a potentially large mismatch between historical and future distributions of events. The interpolation of the damage curve, as described in section 2.2.3, might lead to very high uncertainties that need to be taken into account when interpreting the data.







\section{Inundation}

\subsection{Hazard models}
Inundation is modelled as an acute risk using the approach of Section~\ref{SubSec:AcuteAssetImpactModel}. Hazard event models compatible with this method provide inundation depths for different annual probabilities of occurrence -- or equivalently return periods. The need for sufficient granularity in the set of return periods is discussed in \cite{WardEtAl:2011}. 

Inundation hazards are incorporated into physical risk calculations using the World Resource Institute (WRI) Aqueduct flood model \cite{WardEtAl:2020} which has relatively high return-period granularity. This is based on the global modelling approach of \cite{WardEtAl:2013}.

{\textcolor{red}{\emph{[Discuss and include refs for approaches based on flooded area?]}}}

\subsection{Vulnerability models}
Notable damage models for real estate assets include the FEMA FAST `HAZUS' model \cite{ScawthornEtAl:2006} and an European Commission Joint Research Centre (JRC) model \cite{HuizingaEtAl:2017}. The latter is implemented in the \emph{physrisk} library. 
 

\section{Heat}

Heat is classified as both a chronic and an acute hazard.  For example, increased average temperature in a particular area can lower average productivity from labour or make the area less desirable as a place to live, lowering real estate prices. We classify these as risks from chronic hazards. Heat waves are examples of acute hazard events; a period of particularly high temperature might lead to the complete suspension of industrial activity.

Multiple indexes for quantifying heat hazards have been suggested and multiple approaches for the modelling of acute events are present in the literature, e.g. \cite{MazdiyasniEtAl:2019}. Similarly, various methods for modelling the vulnerability to heat hazards have been suggested. Analyses of heat wave events are commonly based on Global and Regional Circulation Model (GCM and RCM) outputs \cite{DosioEtAl:2018}. In \cite{Christidis:2021} and \cite{Christidis:2013} the authors analyse ensembles of CMIP6 simulations with and without anthropogenic forcings in order to determine if extreme heat events are attributable to (anthropogenic) climate change. Such attribution analysis is based in part on finding return periods of events (see also \cite{StottEtAl:2016}). This estimation of return periods for events is directly applicable to acute hazard models.  

In order to support a wide range of hazard and vulnerability models,  \emph{physrisk} includes the derivation of heat statistics from CMIP6 data\footnote{This is somewhat in contrast to the use of the Aqueduct model of \cite{WardEtAl:2020} for modelling inundation where the complete hazard model is used as-is within \emph{physrisk} -- albeit reformatted to handle efficiently the access patterns needed for physical risk calculations.}. 

\subsection{Hazard Models}

\subsubsection{Chronic hazard models}
As mentioned above, hazard models and vulnerability models are closely coupled. \cite{ZhangAndShindell:2021} describes the `GZN' (Graff-Zivin and Neidell) and `WBGT' (WetBulb Globe Temperature methodology) methods. The statistics required for these methods are derived using bias-corrected and down-scaled data sets. There are multiple sources of data suitable for the estimation of the required statistics, notably the NEX-GDDP-NASA set \cite{ThrasherEtAl:2022}.



\subsubsection{Acute hazard models}
Acute hazard modelling approaches are based on calculating return periods of events in a way analogous to acute inundation models. The calculation of return periods from data sets presents a statistical challenge, dealt with for example by \cite{MentaschiEtAl:2016}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% Changes - Heat Vulnerability Model %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Heat Vulnerability Model}

\label{SubSec:HeatVulnerabilityModel}

\subsubsection{Impact of Temperature on labour productivity}

The Heat vulnerability model presented in this section is based on the approach introduced in 'Temperature And Work: Time allocated to work under varying climate and labor market conditions' (2021)\cite{TemperatureAndWork:2021}. The paper uses survey data to estimate labour allocation decisions in the United States (US) based on temperature. It does not extend the analysis beyond the US. It replicates previous research done (original GZN method \cite{TemperatureAndTheAllocationofTime:2014}.) whilst extending the period of data used and adding an assessment based on the economic cycle: the main innovation is that it includes the economic cycle by splitting out the 2008 financial crisis first through segmented regressions and following them by using an indicator variable: the non-recession period is 2003-2007 and 2015-2018. The Great Recessions period is 2008-2014. The methodology in only applied to climate exposed sectors: agriculture, forestry, fishing and hunting, construction, mining, and transportation and utilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% to add in the references: https://github.com/os-climate/physrisk/blob/main/methodology/PhysicalRiskMethodologyBibliography.bib
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% Original GLZ model
%@article{TemperatureAndTheAllocationofTime:2014,
%  title={Temperature and the Allocation of Time: Implications for Climate Change},
%  author={Matthew Neidell, Joshua Graff Zivin},
%  journal={Journal of Labor Economics},
%  volume={32},
%  number={1},
%  pages={1--26},
%  year={2006},
%  publisher={The University of Chicago Press on behalf of the Society of Labor Economists and the NORC at the University of Chicago}
%}

%%%%% Temperature And Work artcle - which starts from original GZL model
%@article{TemperatureAndWork:2021,
%  title={Temperature and work: Time allocated to work under varying climate and labor market conditions},
%  author={Matthew Neidell, Joshua Graff Zivin, Megan Sheahan, Jacqueline Willwerth, Charles Fant, Marcus Sarofim, Jeremy Martinich},
%  journal={PLOS ONE},
%  year={2021},
%}
The paper main conclusions are:
\begin{itemize}
    \item A statistically significant impact of 2.6 minutes lost per degree of temperature above $90^\circ F$ during normal economic periods and no relationship during a recession. This result is converted into the Celsius scale as \emph{physrisk} decided to take that scale as a reference. The 2.6 minutes is multiplied by a scaling factor of 1.8, which returns \textbf{an impact of 4.7 minutes lost per degree of temperature above $32.2^\circ C$}. 
    \item When using an indicator variable and linear regression the estimated impact was 5.6 minutes under Fahrenheit scale (respectively 10.08 minutes under Celsius scale) during normal economic periods, but the parameter was insignificant. 
    \item No relationship between temperature and work allocation with temperatures below  $32.2^\circ C$. 
    \item Focus on labour allocation decisions, it does not account for other impacts such as reducing productivity. 
\end{itemize}

While the results are relevant and significant, is it important to highlight the following disclaimers on the results:
\begin{itemize}
    \item There are some questions of the reliability of the forecasts in the long run as climate change will likely result in structural changes: on a long-term basis, adaptive solutions might be considered by people to adjust their work productivity with respect to temperature rise.
    \item The conclusion holds for US (so maybe also for the EU as well other developed countries), but not for developing countries which experience more economic turmoil periods.
\end{itemize}

The paper attempts to estimate economic cost (assuming the impact of 4.7 minutes lost per degree of temperature above $32.2^\circ C$ during normal economic periods) however it only focuses on the direct costs and does not account for feedback effects (reducing the labour productivity will result in a decrease of the products available, wages, demand, etc.). Figure \ref{fig:economiccost} provides the results, as extracted from the paper:
\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.6]{economic cost.png}
    \caption{Economic cost}
    \label{fig:economiccost}
\end{figure}

Last but not least, it is worth to note that the results of this paper are interpreted as the impact of \textbf{chronic increase in temperature}.
%NEW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The GZN hazard  model is based on the projections of the climate variable 'daily maximum temperature'. The projected data spans for 20 years. Then, the following statistical processing is used to compute the daily cooling degree days: if the daily number of degrees is higher than $32.2^\circ C$ then the cooling degree days is equal to the number of degrees, and 0 if not. Once the 20-year projected time series of daily cooling degree days is computed, the indicator of the GZN hazard data is calculated as an annual average of the cooling degree days (over all the days within the 20-year period). That indicator will be used afterwards as an input in the impact function to compute the number of minutes of labour productivity loss. Figure \ref{fig:GZL-Hazard} provides a summary of the methodology:

\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.8]{GZL-Hazard.PNG}
    \caption{Cooling degree days and labour availability}
    \label{fig:GZL-Hazard}
\end{figure}


\subsubsection{Uncertainty around the vulnerability Heat model}

\paragraph{Overview}
The assessment is based on the research performed by Zhang and Shindel, which reviews the uncertainty in the heat risk literature \cite{ZhangAndShindell:2021}. This paper provides context around the uncertainty that exists in the result discussed in previous section, which is mainly explained by the methodology used for the estimation of the impact of temperature on labour productivity. 
\begin{itemize}
    \item \cite{ZhangAndShindell:2021} provides an analysis of the \textbf{differential forecasts} between using the \textbf{GZN method} (as reference to Graff-Zivin and Neidell) documented in \cite{TemperatureAndWork:2021}, versus the WetBulb Globe Temperature methodology (\textbf{WBGT method}) which includes other climate factors in addition to temperature: humidity, wind speed and heat radiation -- figure \ref{fig:WBGT} \footnote{Extracted from \cite{ZhangAndShindell:2021} -- p.4} provides the WBGT detailed approach. 
    \item Another major source of differentiation is that the GZN method focuses on changes in labour allocation decisions while the WBGT method focuses on the physiological impacts of rising temperatures.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics{WBGT.png}
    \caption{WBGT method}
    \label{fig:WBGT}
\end{figure}

The $\alpha1$ and $\alpha2$ are the parameters derived for the three different work intensities: light (24.64, 22.72), medium (32.98, 17.81), and heavy (30.94, 16.64) work.
\begin{itemize}
    \item Light work includes all service sectors, such as trade, all retail sales, wholesale trade and commission trade, hotels and restaurants, repairs of motor vehicles and personal and household goods, retail sale of automotive fuel, post and telecom, financial services, insurance, recreational and service activities, public administration, dwellings real estate, and other businesses.
    \item Medium work includes food, textile and wood, machinery and electronic equipment, and other industries and transport sectors.
    \item Heavy work includes agriculture, forestry and fishery, extraction sectors, and construction sectors. These are the sectors considered in the GZN model methodology, which are the climate exposed sectors. Hence in this study, $\alpha1$ and $\alpha2$ of the heavy work intensity are applied $(30.94, 16.64)$.
\end{itemize}



Note that there are differences in the functional forms applied in the original GZN method \cite{TemperatureAndTheAllocationofTime:2014} and the approach presented in \cite{TemperatureAndWork:2021}, with original GZN method using one linear regression with dummy variables for temperature buckets, while \cite{TemperatureAndWork:2021} uses multiple linear regressions with one variable reflecting the breach of the maximum temperature around anchor points (less than $70^\circ F$, $90^\circ F$, $90^\circ F$ and above). The WBGT methodology uses a non-linear function to relate labour loss to the WBGT consolidated measure.   

Figure \ref{fig:CostsByRCP} \footnote{Extracted from \cite{ZhangAndShindell:2021} -- p.11} provides the forecasts of labour lost millions of 2016 USD (constant USD value). Most notably the original GZN produces more optimistic forecasts of the cost of labour lost (lower) than the WBGT method. Note that RCP8.5\footnote{The Intergovernmental Panel on Climate Chance modelling are based on representative concentration pathways (RCPs), which represent different emissions projections under basic, plausible economic and social assumptions, while staying within physical constraints. RPCs are constructed by back-calculating the amount of emissions that would result in a given amount of radiative forcing (which is the difference between solar radiation (energy) absorbed by the Earth and energy radiated back into the space) that would then result in a given amount of warming} refers to the scenario of high emissions and RCP4.5 refers to the scenario of moderate emissions.

\begin{figure}[h]
    \centering
    \includegraphics{CostsByRCP.png}
    \caption{Costs By RCP under GLZ method versus WBGT method}
    \label{fig:CostsByRCP}
\end{figure}



\paragraph{Detailed approach: vulnerability around GLZ model}

There are 2 identified areas where uncertainty in the forecasts in \cite{TemperatureAndWork:2021} paper: the economic cycle and the model parameter uncertainty.
\newline
The \textbf{economic cycle} is one area where uncertainty exists in the forecasts. 
The paper shows that labour allocation decisions are sensitive to where in the economic cycle the US is; during a recession there does not appear to be a relationship between labour allocation and temperature. In order to measure the uncertainty explained by the economic cycle, one might consider the probability of a recession as a Bernoulli random variable with a probability p. Based on this there are two possibly approaches. A first approach is a Monte-Carlo like approach where one can randomly sample the 1, 0 value whether a recession occurs at each period on a time path. A second approach would be to use the expected value of the probability of the recession and estimate the impact as: 
\begin{equation}
    \label{Eq:economiccycle}
    	Forecasted \ Minutes \ Lost = p \times 0 + (1-p) \times EV(X)
\end{equation}
Where $X$ is the variable that refers to Minutes Lost during normal economic cycle
The second approach is attractive in its simplicity and ensuring the model does not lose its focus. 
One concern is that the probability of recession, in reality, may be related to the realisation of climate related risks. Hence, there might be a over/under estimation of the probability unless the impact of the climate risk is also considered. Historical model estimated recession probabilities can be sourced from \cite{SmoothedU.S.RecessionProbabilities:2022}. However, we did not go further in the measurement of the uncertainty explained by the economic cycle because it requires us to focus on the modelling of another parameter ($p$, the probability of the recession to occur), which is not the purpose of this work. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% to add in the references: https://github.com/os-climate/physrisk/blob/main/methodology/PhysicalRiskMethodologyBibliography.bib
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%@article{SmoothedU.S.RecessionProbabilities:2022,
%  title={Smoothed U.S. Recession Probabilities},
%  author={Jeremy Piger},
%  publisher={University of Oregon}
%  year={2022},
%}
Instead, we focus on the \textbf{model parameter uncertainty approach}. In \cite{ZhangAndShindell:2021}, there is a linear relationship between temperature and work minutes lost ($\beta$),and a constraint is applied to ensure that total time allocation sums to 24 hours, which returns a non-linear regression model at the end. Given that the main coefficient of interest is denoted $\beta$, an inference is applied assuming that $\beta$ follows a $Student-T$ distribution: 
\begin{equation}
 \label{Eq:uncertaintyStudentT}   
 d\beta \sim T(\beta, SE, N-K)
\end{equation}
Where $SE$ is the Standard Error of the coefficient and $(N-K)$ is the number of degrees of freedom, $N$ is the number of observations and $K$ is the number of model parameters. Hence, one can estimate the coefficients of the confidence interval (CI) at a given level of confidence $CI\%$: 
\begin{equation}
 \label{Eq:CIStudent}   
 d\beta_{CI\%} = \beta \pm T(p) \times SE
\end{equation}
Where $T(p)$ donates the probability density function (pdf) of the student $T$ distribution with probability $p$ which corresponds to the $CI\%$, the standard error $SE = 2.23 \ min$ and $\beta =  - 4.68$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%START HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure \ref{fig:GZL-Vulnerability} shows the uncertainly around the GZN vulnerability model, based on the assumptions above, at $95\%$ confidence level\footnote{In order to compute the figure, N is taken from the paper \cite{TemperatureAndWork:2021}: N 11,732, and K is assumed to be 0 as it is very small relatively to N. One could do further researches to check the exact number of K, but that will not impact the results. K includes the number of control variables which provide information on demographic data -- age, gender, education, employment status, income, etc. -- and climate variables such as level of precipitation and snow.}.
\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.8]{GZL-Vulnerability.png}
    \caption{Estimate of daily minutes of labour loss}
    \label{fig:GZL-Vulnerability}
\end{figure}


As the degrees of freedom increases, the t distributions converge to the standard normal. Therefore, for simplicity reasons in the context of this work, it is assumed that that the daily labour productivity impact $\beta$ can be measured as:
\begin{equation}
    \label{Eq:uncertainty1}
    	\beta \sim \mathcal{N}(m,\,SE^{2})\,\ where \ m = 4.68 \ min \ and \ SE = 2.23 \ min
\end{equation}


As an example, consider an $1.5^\circ C$ degree day increase in the temperature. We can then multiply through the normal distribution as shown below: 

%The buckets of the daily maximum temperature above $32.2^\circ C$  are defined as an incremental increase of $1.5^\circ %C$, starting from $0^\circ C$ to $18^\circ C$: $\begin{pmatrix} 0 & 1.5 & 3 & ... & 18 \end{pmatrix}$. For a $1.5^\circ %C$ daily temperature increase, the uncertainty around the daily labour productivity impact is given by the following %normal distribution:
\begin{equation}
    \label{Eq:uncertainty2}
    1.5 \times \beta \sim \mathcal{N}(1.5 \times \beta,\, (1.5 \times SE)^{2})\, 
\end{equation}

This can be generalised for a degree day increase of x using the following distribution: 


\begin{equation}
    \label{Eq:uncertainty3}
    x \times \beta \sim \mathcal{N}(x \times \beta,\, (x \times SE)^{2})\, 
\end{equation}
For example, there is 1\% chance that the lost labour productivity exceeds $29.6$ minutes per day if the maximum daily temperature exceeds at $32.2^\circ C$ by $3^\circ C$. This number is computed as $\mathcal{N}^{-1} (m',SE'^2)(1\%)$ where $m'= 3 \times m = -14.013 \ min$ and $SE'= 3 \times SE = 6.69 \ min$. The $99\%CI$ of the lost labour productivity per day if the maximum daily temperature exceeds at $32.2^\circ C$ by $3^\circ C$ is $[+1.6 \ min, -29.6 \ min]$.

%Adding point regarding impact as a percentage of total labour 


The estimated loss of labour time is then transformed into a percentage estimate:

\begin{equation}
    \label{Eq:uncertainty3}
    Estimated \ loss \ of \ labour \ time \ (\%)=  \frac{Labour \ lost}{Total \ minutes \ worked \ in \ a \  year}\
\end{equation}
For this the figures reported by the Organisation for Economic Co-operation and Development (OECD) are used, specifically the 2021 estimate for the United States of America. This is to ensure the alignment with the region where the labour parameters are estimated in. Buckets of labour lost are defined within the range of no impact to 100\%, with estimated marginal probabilities for each bucket returned.


The main weakness of the Temperature and Work article (which is more likely to be close to the original GZN method than to the WBGT method) is that it does not take into account of higher levels of optimism in the original GZN method which was noted in figure \ref{fig:CostsByRCP}. Hence, the results might be underestimating the actual impact. The next paragraph explores the vulnerability model around the WBGT method and its uncertainty.

\paragraph{Detailed approach: vulnerability around WBGT model}

%NEW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The WBGT hazard model, as show in figure \ref{fig:WBGT-Hazard}, is based on the projections of the climate variables 'daily temperature' and 'daily humidity'. The projected data spans for 20 years. Then, the computations provided in figure \ref{fig:WBGT} are used to compute the daily WBGT, which is considered as the indicator of the WBGT hazard data that will be afterwards considered as an input in the impact function to get the daily Work Ability (WA) projections over the 20-year period. Finally, the impact deriving from the WBGT model is computed as the annual average of the daily projected WA.
\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.8]{WBGT-Hazard.PNG}
    \caption{WBGT and labour availability}
    \label{fig:WBGT-Hazard}
\end{figure}
Note that another indicator deriving from the WBGT hazard model is the Work Loss (WL), which is computed as $WL = 1 - WA$. It is another way to do the assessment but leads to same results. 
\newline
The aggregation of the outputs of the GZN vulnerability model (minutes of productivity labour loss) and the WBGT vulnerability model (WA), returns the effective number of working hours, which represents a way to measure the uncertainty around the GZN vulnerability model. Figure \ref{fig:Aggregation} provides the aggregation process and results:
\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.8]{Aggregated Hazard WBGT GZN.PNG}
    \caption{Aggregation of GZN model and WBGT model}
    \label{fig:Aggregation}
\end{figure}
If the WL was used instead of the WA, then it will be multiplied by the hours worked derived from the GZN model to get the annual total labor loss due to heat.

Given the modelling assumptions around the parameters used to compute the WA in the WBGT model, it is important to measure the uncertainty around these parameters, $\alpha1$ and $\alpha2$, which depend on the work intensities. 

The source paper provides the parameters $\alpha1$ and $\alpha2$ for three different work intensities low, medium and high with industries mapped to each intensity. These categories are broad and do not account for variance within and industry and between industries in the same category. To account for this uncertainty the WBGT approach was adjusted to include uncertainty around the industry.

Consider an asset which is market as in a high risk sector. We assume that the work ability is uniformly distributed with a mean equal to the $WA_H$ and a floor (a) and ceiling (b) equidistant from the mean. We assume that the floor a is halfway between $WA_H$ and $WA_M$. So a and b can be estimated based on the below formulae: 

\begin{equation}
    \label{Eq:WBGT_Floor}
     a = WA_H - \frac{WA_H - WA_M}{2}\,
\end{equation}

\begin{equation}
    \label{Eq:WBGT_Ceil}
     b = WA_H + \frac{WA_H - WA_M}{2}\,
\end{equation}

And the WBGT work ability can be represented based on the below formula: 

\begin{equation}
    \label{Eq:WBGT_Uniform}
     WA \sim \mathcal{U}(a ,b)\, 
\end{equation}
 
With this we can estimate the variance of the WBGT work ability using the standard formula for the variance of a Uniform distribution:

\begin{equation}
    \label{Eq:WBGT_Variance}
     Var(WBGT) = \frac{(a-b)^2}{ 12}
\end{equation}

In order to get to a final work ability we multiple the output of the GZN model by the ouput of the WBGT model.

\begin{equation}
    \label{Eq:Final_Mean_WA}
     Effective Work = (1- Estimated \ loss \ of \ labour \ time ) \times WA_H
\end{equation}

As we have uncertainty both for the GZN component as well as the WBGT component we need to estimate a joint variance. We assume that the epistemic uncertainty in the GZN model is uncorrelated with the variance in the sector uncertainty of the work ability measure. To estimate the variance of the product of both the GZN and WBGT components we using the following formula:

\begin{equation}
\begin{aligned}
    \label{Eq:Var_Joint}
     Var(EffectiveWork) = (1- Estimated \ loss \ of \ labour \ time )^2 \times Var(WBGT) + \\
      WA_H^2 \times Var(GZN) +  var(WBGT) \times Var(GZN) 
      \end{aligned}
\end{equation}

To get the final impact we assume that the product of the two variables are normally distributed. The final work ability distribution can be represented as below:

\begin{equation}
\begin{aligned}
    \label{Eq:Final_Result_Heat}
     N(EffectiveWork, Var(Effective Work))
      \end{aligned}
\end{equation}

\clearpage
\printglossaries
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{Physical Risk Methodology Bibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{plain}
\bibliographystyle{acm}
%\bibliographystyle{agsm}



\end{document}
