import os
from pathlib import PurePath
from typing import List
import numpy as np
import pandas as pd
from pint import UnitRegistry
import s3fs

from physrisk.data.utilities import load_credentials
from physrisk.vulnerability_models.config_based_impact_curves import (
    VulnerabilityConfigItem,
)
from physrisk.vulnerability_models.configuration.config_builders import (
    ConfigBuilder,
    ConfigBuilderBase,
)

ureg = UnitRegistry()


class ConfigBuilderHazusWind(ConfigBuilderBase, ConfigBuilder):
    def build_config(self):
        """Create vulnerability config for flood based on the EU JRC global depth-damage functions.
        This replaces the notebook implementation and sets the pattern for code-based generation of the config.
        """
        # we cache the dataframe for speed: takes over a minute to load from Excel
        df = self.cached_dataframe()
        grouped_df_terrain_id = self._grouped_data_frame(
            df, groupby=["building_type", "damage_type", "terrain_id"]
        )
        config_items_terrain_ids = self._config_items(
            grouped_df_terrain_id, attributes=["building_type", "terrain_id"]
        )
        grouped_df = self._grouped_data_frame(
            df, groupby=["building_type", "damage_type"]
        )
        config_items = self._config_items(grouped_df, attributes=["building_type"])
        return config_items + config_items_terrain_ids

    def download_inputs(self):
        load_credentials()
        filename = "HazusWindDamFunctions_Hazus61.xlsx"
        s3 = s3fs.S3FileSystem(
            anon=False,
            key=os.environ.get("OSC_S3_ACCESS_KEY_DEV", None),
            secret=os.environ.get("OSC_S3_SECRET_KEY_DEV", None),
        )
        s3.download(
            str(
                PurePath(os.environ.get("OSC_S3_BUCKET_DEV"))
                / "vulnerability"
                / filename
            ),
            str(self.source_dir / "wind_hazus" / filename),
        )

        # or we can use boto3, e.g.:
        # s3_client = boto3.client(
        #    "s3",
        #    aws_access_key_id=os.environ.get("OSC_S3_ACCESS_KEY_DEV", None),
        #    aws_secret_access_key=os.environ.get("OSC_S3_SECRET_KEY_DEV", None)
        # )
        # with open(filename, 'wb') as f:
        #    s3_client.download_fileobj(os.environ.get("OSC_S3_BUCKET_DEV"), f"vulnerability/{filename}", f)

    def cached_dataframe(self):
        cached_df_path = (
            self.source_dir / "wind_hazus" / "HazusWindDamFunctions_Hazus61.csv"
        )
        if not cached_df_path.exists():
            df = self._load_data_frame()
            df.to_csv(cached_df_path)
        df = pd.read_csv(cached_df_path)
        return df

    def _config_items(
        self, df: pd.DataFrame, attributes: List[str] = ["building_type", "terrain_id"]
    ):
        units = "mph"
        df["points_y"] = df.apply(
            lambda row: [
                float(row[key]) for key in row.keys() if key.startswith("dmg_")
            ],
            result_type="reduce",
            axis=1,
        )
        if np.all(
            [
                column.endswith(units)
                for column in df.columns
                if column.startswith("dmg_")
            ]
        ):
            points_x = ureg.convert(
                np.array(
                    [
                        float(column[4 : -len(units)])
                        for column in df.columns
                        if column.startswith("dmg_")
                    ]
                ),
                units,
                "m/s",
            )
        hazard_class = "Wind"
        indicator_id = "max_speed"
        indicator_units = "m/s"
        impact_id = "damage"
        attribute_convert = {"building_type": "hazus_wind_spec_build_type"}

        def asset_identifier(row):
            return ",".join(
                [attribute_convert.get(a, a) + "=" + str(row[a]) for a in attributes]
            )

        config_items = list(
            df[df["damage_type"] == "structure"]
            .apply(
                lambda row: VulnerabilityConfigItem(
                    hazard_class=hazard_class,
                    asset_class="Asset",
                    asset_identifier=asset_identifier(row),
                    indicator_id=indicator_id,
                    indicator_units=indicator_units,
                    impact_id=impact_id,
                    impact_units=None,
                    curve_type="indicator/piecewise_linear",
                    points_x=points_x,
                    points_y=[
                        float(row[key]) for key in row.keys() if key.startswith("dmg_")
                    ],
                    points_z=None,
                    cap_of_points_x=None,
                    cap_of_points_y=None,
                    activation_of_points_x=None,
                ),
                axis=1,
            )
            .values
        )
        return config_items

    def _grouped_data_frame(
        self,
        df: pd.DataFrame,
        groupby: List[str] = ["building_type", "damage_type", "terrain_id"],
    ):
        df_group = df.groupby(groupby).mean().reset_index()
        df_group = df_group.apply(
            lambda row: (
                {
                    key: row[key] / 360.0 if key.upper().startswith("WS") else row[key]
                    for key in row.index
                }
                if row["damage_type"] == "downtime"
                else row
            ),
            axis=1,
        )
        df_group = df_group.rename(
            columns={
                column: "dmg_" + column[2:] + "mph"
                for column in df_group.columns
                if column.upper().startswith("WS")
            }
        )
        return df_group

    def _load_data_frame(self):
        file_path = (
            self.source_dir / "wind_hazus" / "HazusWindDamFunctions_Hazus61.xlsx"
        )
        # wind building types
        wbt_df = pd.read_excel(io=str(file_path), sheet_name="huListOfWindBldgTypes")
        wbt_df = wbt_df.rename(columns={"sbtName": "building_type"})

        damage_df = pd.read_excel(
            io=str(file_path), sheet_name="huDamLossFunDescription"
        )
        damage_df = damage_df.rename(columns={"DamLossClass": "damage_type"})
        damage_df["damage_type"] = damage_df["damage_type"].apply(
            lambda x: x.replace(" ", "").replace("\t", "")
        )

        df = pd.read_excel(io=str(file_path), sheet_name="huDamLossFun")
        df = df.merge(damage_df, on="DamLossDescID", how="left")
        df = df[df["damage_type"].isin(["Building", "Content", "LossOfUse"])]
        df = df.merge(wbt_df, on="wbID", how="left")
        df = df.rename(columns={"TERRAINID": "terrain_id"})
        df = df[
            ["building_type", "damage_type", "terrain_id"]
            + [column for column in df.columns if column.upper().startswith("WS")]
        ]
        df["damage_type"] = df["damage_type"].apply(
            lambda key: (
                "structure"
                if key.lower() == "building"
                else ("contents" if key.lower() == "content" else "downtime")
            )
        )
        return df

    # def _occupancy_sbt_blend(self):
    #     file_path = (
    #         self.source_dir / "wind_hazus" / "HazusWindDamFunctions_Hazus61.xlsx"
    #     )
    #     # wind building types
    #     wbt_df = pd.read_excel(io=str(file_path), sheet_name="huListOfWindBldgTypes")


class ConfigBuilderHazusFlood(ConfigBuilderBase, ConfigBuilder):
    def _load_data_frame(self):
        units_for_depth = {"m": 1.0, "ft": 3.28084}
        hazus_path = (
            self.source_dir / "wind_hazus" / "HazusWindDamFunctions_Hazus61.xlsx"
        )
        flood_df = pd.DataFrame(columns=["hazus_id", "points_x", "points_y"])
        if hazus_path.exists():
            hazus_file = pd.ExcelFile(hazus_path)
            sheet_names = [
                sn
                for sn in hazus_file.sheet_names
                if sn.startswith("flBldg")
                and (sn.endswith("DmgFn") or sn.endswith("DmgFunc"))
            ]
            for sheet_name in sheet_names:
                df = pd.read_excel(io=str(hazus_path), sheet_name=sheet_name)
                df["points_y"] = df.apply(
                    lambda row: [
                        float(row[key]) / 100.0
                        for key in row.keys()
                        if key.startswith("ft")
                    ],
                    result_type="reduce",
                    axis=1,
                )
                points_x = [
                    np.round(
                        (
                            -float(column[2:-1])
                            if column.endswith("m")
                            else float(column[2:])
                        )
                        / units_for_depth[column[:2]],
                        4,
                    )
                    for column in df.columns
                    if column.startswith("ft")
                ]
                df["points_x"] = df["points_y"].apply(
                    lambda _, points_x=points_x: points_x
                )
                assert (
                    len(
                        [
                            column
                            for column in df.columns
                            if column.lower().endswith("dmgfnid")
                        ]
                    )
                    == 1
                )
                column = [
                    column
                    for column in df.columns
                    if column.lower().endswith("dmgfnid")
                ][0]
                key = column[: -len("dmgfnid")].upper()
                if key == "BLDG":
                    key = "structure"
                elif key == "CONT":
                    key = "contents"
                elif key == "INV":
                    key = "inventory"
                assert key in ["structure", "contents", "inventory"]
                df["hazus_id"] = df[column].apply(
                    lambda id, key=key: key + "_" + str(id)
                )
                df = df[["hazus_id", "points_x", "points_y"]]
                flood_df = pd.concat(
                    [flood_df[~flood_df["hazus_id"].isin(df["hazus_id"])], df]
                )
            return flood_df
        else:
            raise FileNotFoundError(
                f"Hazus flood data file not found at {hazus_path}. "
                "Please download the file and place it in the correct directory."
            )
