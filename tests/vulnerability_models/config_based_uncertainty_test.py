from typing import Optional
from numba import float64, njit
from numba.experimental import jitclass
import numpy as np
import pytest

from physrisk.kernel.vulnerability_model import checked_beta_distrib


@pytest.mark.skip("in dev")
def test_vuln_uncertainty():
    hazard_indicator = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0]
    impact_mean = [0.0, 0.327, 0.494, 0.617, 0.721, 0.870, 0.931, 0.984, 1.0]
    impact_stddev = [0.0, 0.25, 0.22, 0.21, 0.21, 0.17, 0.12, 0.048, 0.0]
    cdf = CDFBasedVulnerabilityFunction.from_stddev(
        impact_mean, impact_stddev, CDFBasedVulnerabilityFunction._default_impact_grid
    )
    float("inf") - 4


class CDFBasedVulnerabilityFunction:
    """In general the vulnerability curve specifies for each hazard intensity level a
    cumulative probability density function.
    More precisely:
    The hazard intensity values, :math:`x_i`, are given, :math:`i \in [1 \dots n]`.
    .. math:: F_i(y) = \mathbb{P}(Y \leq y|x_i)

    The CDF, :math:`F_i(y)`, is given for points :math:`y_j`, :math:`j \in [1 \dots m]`.
    .. math::
        F_{ij} = \mathbb{P}(Y \leq y_j|x_i)
        x = [x_1, x_2, \dots, x_n ]
        y = [y_1, y_2, \dots, y_m ]
        z = [[F_{11}, F_{12}, \dots, F_{1m}], [F_{21}, F_{12}, \dots, F_{2m}], \dots, [F_{n1}, F_{n2}, \dots, F_{nm}]]

    Two cases:
    1) Hazard intensity, :math:`x'` is known (i.e. a drawing from an event set)
    Here the CDF is interpolated from the CDFs bounding the :math:`x'`:

    .. math:: F_{x'}(y_j) = \frac{x_{k+1} - x'}{x_{k+1} - x_k} F_{kj} + \frac{x' - x_k}{x_{k+1} - x_k} F_{(k+1)j}

    2) Probability is known within a range :math:(a, b).
    Here the CDF within the interval is approximated by the CDF interpolated at the midpoint math:`x' = \frac{a + b}{2}`.
    """

    _default_impact_grid = np.array(
        [0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    )

    __slots__ = "x", "y", "z", "impact_grid", "kind"

    def __init__(
        self,
        x: np.ndarray,
        y: np.ndarray,
        z: Optional[np.ndarray],
        kind: str = "deterministic",
        impact_grid: Optional[np.ndarray] = _default_impact_grid,
    ):
        """Initialise a CDF

        Args:
            x (np.ndarray): array_like, (`n`,).
                            Hazard indicator values.
            y (np.ndarray): array_like, (`m`,).
                            Impact values if kind is 'deterministic', (`n`, ).
                            Mean impact values if kind is 'beta' or 'trunc_gauss', (`n`, ).
                            Impact values for CDF specified in `z` if kind is 'cdf', (`m`, )
            z (Optional[np.ndarray]): array_like.
                            Standard deviation of impact values if `kind` is 'beta' or 'trunc_gauss',
                            (`n`, ).
                            If `kind` is `cdf` then `z[i, j]` is the probability that if
                            the hazard indicator value is given by `x[i]` the impact is
                            `<= y[j]`, (`n`, `m`).
            kind (str): 'deterministic', 'beta', 'trunc_gaussian', 'cdf'. Defaults to "deterministic".
            impact_grid (np.ndarray): array_like, (`m`,).
                            If kind is 'beta' or 'trunc_gauss', defines the impact values on which
                            the CDF is calculated, otherwise unused.
        """
        if x.ndim != 1 or y.ndim != 1:
            raise ValueError("`x` and `y` must be 1 dimensional.")
        if kind in ["deterministic", "beta", "trunc_gauss"]:
            if len(x) != len(y):
                raise ValueError(f"`x` and `y` must have same length in `{kind}` case.")
            if z is not None:
                raise ValueError(f"`z` value is not used in `{kind}` case.")
        if kind == "cdf":
            if z.ndim != 2 or z.shape[0] != len(x) or z.shape[1] != len(y):
                raise ValueError("`z` should be two dimensional with shape (`n`, `m`).")
            self.cdf = z
        self.x = x
        self.kind = kind
        self.impact_grid = impact_grid

    def interpolate_cdfs(self, hazard_values: np.ndarray):
        """Returns cumulative probabilities for the impact values for the given set of hazard_values.

        Args:
            hazard_values (np.ndarray): _description_
        """
        left = np.searchsorted(self.x, hazard_values, side="left")
        right = left + 1
        cdfs = (
            (self.x[right] - hazard_values) * self.cdf[left, :]
            + (hazard_values - self.x[left]) * self.cdf[right, :]
        ) / (self.x[right] - self.x[left])
        cdfs[left == len(self.x), :] = cdfs[len(self.x) - 1, :]
        return cdfs

    def cdf(self, intensity: np.ndarray):
        index = np.searchsorted(self.intensity, intensity)
        lower = self.intensity[index]
        upper = self.intensity[index + 1]
        w1 = (intensity - lower) / (upper - lower)

    @staticmethod
    def from_stddev(
        impact_mean: np.ndarray,
        impact_stddev: np.ndarray,
        impact_grid: np.ndarray,
        kind: str,
    ):
        if kind == "beta":
            # this is done just once for curve
            impact_cdfs = [
                checked_beta_distrib(m, s)(impact_grid)
                for m, s in zip(impact_mean, impact_stddev)
            ]
            return impact_cdfs
        else:
            # TODO add support for "truncated_gaussian"
            raise NotImplementedError()


@njit(cache=True)
def sample_from_cumulative_probs(
    values: np.ndarray, cum_probs: np.ndarray, uniforms: np.ndarray
):
    n = cum_probs.shape[0]
    nb_samples = uniforms.shape[1]
    assert uniforms.shape[0] == n
    samples = np.zeros(shape=(n, nb_samples))
    for i in range(n):
        samples[i, :] = np.interp(uniforms[i, :], cum_probs[i, :], values)
    return samples
